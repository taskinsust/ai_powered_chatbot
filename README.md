## ðŸ“˜ LLM-Powered Business Chatbot

### ðŸ§  Overview

A FastAPI-based, on-premises chatbot solution that handles business queries using a **Retrieval-Augmented Generation (RAG)** pipeline and local **LLaMA 3 inference** via `llama-cpp-python`. It supports Bangla and English seamlessly, optimized for modularity, scalability, and data privacy.

---

## ðŸ“¦ Tech Stack

- **FastAPI** â€” Modular backend framework  
- **FAISS** â€” Local vector store for semantic retrieval  
- **Sentence Transformers** â€” Embeddings (multilingual support)  
- **llama-cpp-python** â€” Fast local LLaMA 3 inference  
- **Pydantic / JWT** â€” Auth & request validation  
- **BeautifulSoup, pdfplumber, PyMuPDF** â€” Document parsing  
- **scikit-learn, numpy** â€” Advanced intent classification  

---

## ðŸ§± Architecture Overview

```
Query â†’ IntentClassifier â†’ Embedding â†’ Vector DB â†’ Context Chunks
      â†’ PromptBuilder â†’ LLaMAEngine â†’ Final Response
```

Handles:
- Business queries with RAG + LLM  
- Casual queries with personality  
- Fallbacks gracefully  
- Fully local execution â€” no external API calls

---

## ðŸ§© Modules

| Module                       | Description |
|------------------------------|-------------|
| `app/routes/chat.py`        | Unified `/chat` endpoint |
| `app/classifiers/advanced_intent_classifier.py` | Embedding-based intent detection |
| `app/embeddings/embedding_generator.py` | Embedding engine (bulk + single) |
| `app/retrieval/document_loader.py` | PDF, HTML, JSON loader |
| `app/retrieval/vector_store.py` | FAISS vector DB for chunk search |
| `app/llm/prompt_builder.py` | Formats prompt using user query + context |
| `app/llm/llama_engine.py`   | Runs `.gguf` LLaMA 3 model locally |
| `app/dependencies/`         | Injects singleton vector DB instance |
| `debug/`                    | Test scripts for isolated modules |

---

## ðŸ’¬ Intent Handling

- **Business**: Retrieves chunks â†’ builds prompt â†’ uses LLaMA to answer
- **Casual**: Lightweight prompt â†’ LLaMA responds warmly
- **Fallback**: Polite guidance back to business domain

Intent classifier uses semantic embeddings and cosine similarity for flexible phrasing and regional language support.

---

## ðŸ“‚ Setup Instructions

1. **Install Dependencies**

```bash
pip install -r requirements.txt
```

2. **Download Quantized Model (GGUF)**
Place in `models/` folder and update `model_path` in `llama_engine.py`

3. **Run FastAPI**
```bash
uvicorn app.main:app --reload
```

4. **Run Debug Scripts**
```bash
python -m debug.test_advanced_classifier
```

---

## ðŸ“ˆ Future Enhancements

- Token streaming for smoother UX  
- Product-level semantic search  
- Query logging and feedback loop  
- Context-aware personalization  
- Embedding-based product tagging
